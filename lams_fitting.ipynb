{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yizu-zhang/CSP/blob/master/lams_fitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AWcTW2B0DEu"
      },
      "source": [
        "run these code block **sequentially** use shift+Enter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "qiGdOtp2CmDQ"
      },
      "outputs": [],
      "source": [
        "# parameter\n",
        "\n",
        "BATCH_SIZE = 200\n",
        "EPOCHS = 100\n",
        "train_size = 20000\n",
        "test_size =  1807"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29liKyfiK60k",
        "outputId": "9251281c-746e-41e1-810f-666778760f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.7.0\n",
            "Keras version 2.7.0\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import csv\n",
        "from sklearn.preprocessing import *\n",
        "import os, re, math, json, shutil, pprint\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import IPython.display as display\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "print(\"Keras version \" + keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "tKSmzH6EBBNk"
      },
      "outputs": [],
      "source": [
        "#@title visualization utilities [RUN ME]\n",
        "\"\"\"\n",
        "This cell contains helper functions used for visualization\n",
        "and downloads only. You can skip reading it. There is very\n",
        "little useful Keras/Tensorflow code here.\n",
        "\"\"\"\n",
        "\n",
        "# Matplotlib config\n",
        "plt.ioff()\n",
        "plt.rc('image', cmap='gray_r')\n",
        "plt.rc('grid', linewidth=1)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0', figsize=(16,9))\n",
        "# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "\n",
        "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
        "  \n",
        "  # get one batch from each: 10000 validation digits, N training digits\n",
        "  batch_train_ds = training_dataset.apply(tf.data.experimental.unbatch()).batch(N)\n",
        "  \n",
        "  # eager execution: loop through datasets normally\n",
        "  if tf.executing_eagerly():\n",
        "    for validation_digits, validation_labels in validation_dataset:\n",
        "      validation_digits = validation_digits.numpy()\n",
        "      validation_labels = validation_labels.numpy()\n",
        "      break\n",
        "    for training_digits, training_labels in batch_train_ds:\n",
        "      training_digits = training_digits.numpy()\n",
        "      training_labels = training_labels.numpy()\n",
        "      break\n",
        "    \n",
        "  else:\n",
        "    v_images, v_labels = validation_dataset.make_one_shot_iterator().get_next()\n",
        "    t_images, t_labels = batch_train_ds.make_one_shot_iterator().get_next()\n",
        "    # Run once, get one batch. Session.run returns numpy results\n",
        "    with tf.Session() as ses:\n",
        "      (validation_digits, validation_labels,\n",
        "       training_digits, training_labels) = ses.run([v_images, v_labels, t_images, t_labels])\n",
        "  \n",
        "  # these were one-hot encoded in the dataset\n",
        "  validation_labels = np.argmax(validation_labels, axis=1)\n",
        "  training_labels = np.argmax(training_labels, axis=1)\n",
        "  \n",
        "  return (training_digits, training_labels,\n",
        "          validation_digits, validation_labels)\n",
        "\n",
        "# create digits from local fonts for testing\n",
        "def create_digits_from_local_fonts(n):\n",
        "  font_labels = []\n",
        "  img = PIL.Image.new('LA', (28*n, 28), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
        "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
        "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
        "  d = PIL.ImageDraw.Draw(img)\n",
        "  for i in range(n):\n",
        "    font_labels.append(i%10)\n",
        "    d.text((7+i*28,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
        "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
        "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [28, 28*n]), n, axis=1), axis=0), [n, 28*28])\n",
        "  return font_digits, font_labels\n",
        "\n",
        "# utility to display a row of digits with their predictions\n",
        "def display_digits(digits, predictions, labels, title, n):\n",
        "  fig = plt.figure(figsize=(13,3))\n",
        "  digits = np.reshape(digits, [n, 28, 28])\n",
        "  digits = np.swapaxes(digits, 0, 1)\n",
        "  digits = np.reshape(digits, [28, 28*n])\n",
        "  plt.yticks([])\n",
        "  plt.xticks([28*x+14 for x in range(n)], predictions)\n",
        "  plt.grid(b=None)\n",
        "  for i,t in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
        "    if predictions[i] != labels[i]: t.set_color('red') # bad predictions in red\n",
        "  plt.imshow(digits)\n",
        "  plt.grid(None)\n",
        "  plt.title(title)\n",
        "  display.display(fig)\n",
        "  \n",
        "# utility to display multiple rows of digits, sorted by unrecognized/recognized status\n",
        "def display_top_unrecognized(digits, predictions, labels, n, lines):\n",
        "  idx = np.argsort(predictions==labels) # sort order: unrecognized first\n",
        "  for i in range(lines):\n",
        "    display_digits(digits[idx][i*n:(i+1)*n], predictions[idx][i*n:(i+1)*n], labels[idx][i*n:(i+1)*n],\n",
        "                   \"{} sample validation digits out of {} with bad predictions in red and sorted first\".format(n*lines, len(digits)) if i==0 else \"\", n)\n",
        "\n",
        "def plot_learning_rate(lr_func, epochs):\n",
        "  xx = np.arange(epochs+1, dtype=np.float)\n",
        "  y = [lr_decay(x) for x in xx]\n",
        "  fig, ax = plt.subplots(figsize=(9, 6))\n",
        "  ax.set_xlabel('epochs')\n",
        "  ax.set_title('Learning rate\\ndecays from {:0.3g} to {:0.3g}'.format(y[0], y[-2]))\n",
        "  ax.minorticks_on()\n",
        "  ax.grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
        "  ax.grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
        "  ax.step(xx,y, linewidth=3, where='post')\n",
        "  display.display(fig)\n",
        "\n",
        "class PlotTraining(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, sample_rate=1, zoom=1):\n",
        "    self.sample_rate = sample_rate\n",
        "    self.step = 0\n",
        "    self.zoom = zoom\n",
        "    self.steps_per_epoch = 60000//BATCH_SIZE\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.batch_history = {}\n",
        "    self.batch_step = []\n",
        "    self.epoch_history = {}\n",
        "    self.epoch_step = []\n",
        "    self.fig, self.axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "    plt.ioff()\n",
        "\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    if (batch % self.sample_rate) == 0:\n",
        "      self.batch_step.append(self.step)\n",
        "      for k,v in logs.items():\n",
        "        # do not log \"batch\" and \"size\" metrics that do not change\n",
        "        # do not log training accuracy \"acc\"\n",
        "        if k=='batch' or k=='size':# or k=='acc':\n",
        "          continue\n",
        "        self.batch_history.setdefault(k, []).append(v)\n",
        "    self.step += 1\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    plt.close(self.fig)\n",
        "    self.axes[0].cla()\n",
        "    self.axes[1].cla()\n",
        "      \n",
        "#     self.axes[0].set_ylim(0, 1.2/self.zoom)\n",
        "#     self.axes[1].set_ylim(1-1/self.zoom/2, 1+0.1/self.zoom/2)\n",
        "    \n",
        "    self.epoch_step.append(self.step)\n",
        "    for k,v in logs.items():\n",
        "      # only log validation metrics\n",
        "      if not k.startswith('val_'):\n",
        "        continue\n",
        "      self.epoch_history.setdefault(k, []).append(v)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    \n",
        "    for k,v in self.batch_history.items():\n",
        "      self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.batch_step) / self.steps_per_epoch, v, label=k)\n",
        "      \n",
        "    for k,v in self.epoch_history.items():\n",
        "      self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.epoch_step) / self.steps_per_epoch, v, label=k, linewidth=3)\n",
        "      \n",
        "    self.axes[0].legend()\n",
        "    self.axes[1].legend()\n",
        "    self.axes[0].set_xlabel('epochs')\n",
        "    self.axes[1].set_xlabel('epochs')\n",
        "    self.axes[0].minorticks_on()\n",
        "    self.axes[0].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
        "    self.axes[0].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
        "    self.axes[1].minorticks_on()\n",
        "    self.axes[1].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
        "    self.axes[1].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
        "    display.display(self.fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKZNOFr2mwl9"
      },
      "source": [
        "the next part is to add train and test data,\n",
        "click run and upload from local environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "34KSRHjrNWk8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('lam_data.csv', header=None)\n",
        "\n",
        "# shuffle data if necessary\n",
        "# df = df.iloc[0:90000,:]\n",
        "# df = df.sample(frac=1, axis=0).reset_index(drop=False)\n",
        "\n",
        "\n",
        "# # format of train/test data\n",
        "# # [ENTRY dih6 dih10 dih11 dih12 dih13 dih34 \\DeltaUintra(kJ/mol)]\n",
        "\n",
        "train_cord = df.iloc[0:train_size, 1:7].values\n",
        "train_label1 = df.iloc[0:train_size, 7].values\n",
        "train_label1 = np.reshape(train_label1, (train_size, 1))\n",
        "\n",
        "valid_cord = df.iloc[train_size:(train_size+test_size), 1:7].values\n",
        "valid_label1 = df.iloc[train_size:(train_size+test_size),7].values\n",
        "valid_label1 = np.reshape(valid_label1, (test_size, 1))\n",
        "\n",
        "\n",
        "\n",
        "# power transformation of energy\n",
        "# from train data\n",
        "scaler = PowerTransformer(method=\"box-cox\")\n",
        "train_label2 = scaler.fit_transform(train_label1)\n",
        "valid_label2 = scaler.transform(valid_label1)\n",
        "\n",
        "# shift = np.min(train_label2)\n",
        "# print(shift)\n",
        "\n",
        "train_label = train_label2\n",
        "valid_label = valid_label2\n",
        "\n",
        "# df2 = pd.DataFrame(np.vstack([train_label, valid_label]))\n",
        "# df2.to_csv('output_transformed_Uintra.csv', index=False, header=False)\n",
        "# print(np.min(train_label),np.max(train_label), np.min(train_cord), np.max(train_cord), np.mean(train_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Laf1kWvNy01Y"
      },
      "source": [
        "check train and test data dimensions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NahhIBaNo3l",
        "outputId": "44aa5149-0a54-4526-a9cb-f35e8166eafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_cord looks like this: \n",
            " [[ 172.  -30.   60.   60.   60. -120.]\n",
            " [ 172.  -30.   60.   60.   60.    0.]\n",
            " [ 172.  -30.   60.   60.   60.  120.]\n",
            " [ 172.  -30.   60.   60.  180. -120.]\n",
            " [ 172.  -30.   60.   60.  180.    0.]\n",
            " [ 172.  -30.   60.   60.  180.  120.]\n",
            " [ 172.  -30.   60.   60.  300. -120.]\n",
            " [ 172.  -30.   60.   60.  300.    0.]\n",
            " [ 172.  -30.   60.   60.  300.  120.]\n",
            " [ 172.  -30.   60.  180.   60. -120.]]\n"
          ]
        }
      ],
      "source": [
        "train_cord.shape\n",
        "print(\"train_cord looks like this: \\n\",train_cord[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8UBr2vb9IEJ",
        "outputId": "f5e0a770-21ca-41df-d427-a42447ab8484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_label looks like this: \n",
            " [[ 0.24919195]\n",
            " [-0.59209562]\n",
            " [ 0.00855569]\n",
            " [ 0.1569385 ]\n",
            " [-0.75025959]\n",
            " [-0.09784053]\n",
            " [ 0.62064035]\n",
            " [-0.03230892]\n",
            " [ 2.54449966]\n",
            " [ 0.29508016]]\n"
          ]
        }
      ],
      "source": [
        "train_label.shape\n",
        "print(\"train_label looks like this: \\n\",train_label[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEt5yJBMlRRf",
        "outputId": "7cd8356b-03f8-46d1-bda9-f34c7dffac7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1807, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "valid_cord.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX-KyU8clOyn",
        "outputId": "aa90b7ea-8da3-441e-9d1d-82630af591fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1807, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "valid_label.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q56OGxwczCFa"
      },
      "source": [
        "this is the part to set up NN model\n",
        "sequentially we add layers with # of neurons and activation defined\n",
        "\n",
        "the shape of the layers will printed in the next section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "dJZ1J_f5Nvln"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "# elu = keras.layers.ELU(alpha=5.0)\n",
        "network.add(layers.Dense(6, activation='sigmoid', input_shape=(train_cord.shape[1],)))\n",
        "network.add(layers.Dense(36, activation='sigmoid'))\n",
        "network.add(layers.Dense(105, activation='sigmoid'))\n",
        "network.add(layers.Dense(105, activation='sigmoid'))\n",
        "network.add(layers.Dense(36, activation='sigmoid'))\n",
        "network.add(layers.Dense(6, activation='sigmoid'))\n",
        "network.add(layers.Dense(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK1gyR0vzODZ"
      },
      "source": [
        "this is the part to set up your optimizer for fitting the model\n",
        "we use Stochastic gradient descent (please refer to documentation for details)\n",
        "\n",
        "also compile the whole thing with the settings of optimizer, loss function of mean square error, and metrics of mean absolute error\n",
        "\n",
        "set up a cool learning rate feature to control step of optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q7HJyfSRYyJ",
        "outputId": "7dc9dc6d-5880-47e1-cadd-194459670f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_61 (Dense)            (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 36)                252       \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 105)               3885      \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 105)               11130     \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 36)                3816      \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 6)                 222       \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 1)                 7         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,354\n",
            "Trainable params: 19,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "\n",
        "# All parameter gradients will be clipped to\n",
        "# a maximum norm of 1.\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=1e-2,\n",
        "#     decay_steps=10000,\n",
        "#     decay_rate=0.9)\n",
        "# optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
        "                                beta_1=0.9,\n",
        "                                beta_2=0.999,\n",
        "                                epsilon=1e-07,\n",
        "                                amsgrad=False,\n",
        "                                name=\"Adam\")\n",
        "\n",
        "network.compile(optimizer=adam,\n",
        "                loss='mse',\n",
        "                metrics=['mae'])\n",
        "\n",
        "# lr decay function\n",
        "# def lr_decay(epoch):\n",
        "  # return 1 * math.pow(0.97, epoch)\n",
        "\n",
        "# lr schedule callback\n",
        "# lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\n",
        "\n",
        "# important to see what you are doing\n",
        "# plot_learning_rate(lr_decay, EPOCHS)\n",
        "\n",
        "\n",
        "# print model layers\n",
        "network.summary()\n",
        "\n",
        "# utility callback that displays training curves\n",
        "plot_training = PlotTraining(sample_rate=10, zoom=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "x6k-ckZjDCvH",
        "outputId": "65237aea-a8fa-446d-e9ff-2441c503668b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-2bd73884e395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m history = network.fit(train_cord, train_label, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n\u001b[0;32m----> 8\u001b[0;31m                       validation_data=(valid_cord, valid_label), validation_steps=steps_per_epoch, callbacks=[plot_training])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-9bbf2aa3bb3d>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (2,) and (1,)"
          ]
        }
      ],
      "source": [
        "steps_per_epoch = train_size//BATCH_SIZE \n",
        "print(\"Steps per epoch: \", steps_per_epoch)\n",
        "\n",
        "tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=50, verbose=0, \n",
        "                                        mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "history = network.fit(train_cord, train_label, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=(valid_cord, valid_label), validation_steps=steps_per_epoch, callbacks=[plot_training])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1HMp5btwqdt"
      },
      "outputs": [],
      "source": [
        "# for layer in network.layers:\n",
        "#     weights = layer.get_weights()\n",
        "#     print(*(node for node in weights))# ( for node in layer.get_weights())\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import h5py\n",
        "\n",
        "def print_structure(weight_file_path):\n",
        "    \"\"\"\n",
        "    Prints out the structure of HDF5 file.\n",
        "\n",
        "    Args:\n",
        "      weight_file_path (str) : Path to the file to analyze\n",
        "    \"\"\"\n",
        "    f = h5py.File(weight_file_path)\n",
        "    try:\n",
        "        if len(f.attrs.items()):\n",
        "            print(\"{} contains: \".format(weight_file_path))\n",
        "            print(\"Root attributes:\")\n",
        "        for key, value in f.attrs.items():\n",
        "            print(\"  {}: {}\".format(key, value))\n",
        "\n",
        "        if len(f.items())==0:\n",
        "            return \n",
        "\n",
        "        for layer, g in f.items():\n",
        "            print(\"  {}\".format(layer))\n",
        "            print(\"    Attributes:\")\n",
        "            for key, value in g.attrs.items():\n",
        "                print(\"      {}: {}\".format(key, value))\n",
        "\n",
        "            print(\"    Dataset:\")\n",
        "            for p_name in g.keys():\n",
        "                param = g[p_name]\n",
        "                subkeys = param.keys()\n",
        "                for k_name in param.keys():\n",
        "                    print(\"      {}/{}: {}\".format(p_name, k_name, param.get(k_name)[:]))\n",
        "    finally:\n",
        "        f.close()\n",
        "\n",
        "network.save_weights('lams_weights.h5')\n",
        "print_structure('lams_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_Tj2BOjUST-",
        "outputId": "7b11fd70-f20c-4f09-d695-76cfa384e968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0      1     2     3      4      5      6         7\n",
            "0   1  172.0 -30.0  60.0   60.0   60.0 -120.0  2.085973\n",
            "1   2  172.0 -30.0  60.0   60.0   60.0    0.0  1.386732\n",
            "2   3  172.0 -30.0  60.0   60.0   60.0  120.0  1.851317\n",
            "3   4  172.0 -30.0  60.0   60.0  180.0 -120.0  1.992188\n",
            "4   5  172.0 -30.0  60.0   60.0  180.0    0.0  1.287737\n",
            "5   6  172.0 -30.0  60.0   60.0  180.0  120.0  1.757339\n",
            "6   7  172.0 -30.0  60.0   60.0  300.0 -120.0  2.518620\n",
            "7   8  172.0 -30.0  60.0   60.0  300.0    0.0  1.814555\n",
            "8   9  172.0 -30.0  60.0   60.0  300.0  120.0  7.350009\n",
            "9  10  172.0 -30.0  60.0  180.0   60.0 -120.0  2.134506\n",
            "[[1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]\n",
            " [1.6832469]]\n"
          ]
        }
      ],
      "source": [
        "train_pred = network.predict(train_cord)\n",
        "valid_pred = network.predict(valid_cord)\n",
        "\n",
        "train_pred_inverse = scaler.inverse_transform(train_pred)\n",
        "valid_pred_inverse = scaler.inverse_transform(valid_pred)\n",
        "print(df[0:10])\n",
        "print(train_pred_inverse[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "HOWChmh3xsib"
      },
      "outputs": [],
      "source": [
        "df3 = pd.DataFrame(np.vstack([train_pred_inverse,valid_pred_inverse]))\n",
        "df3.to_csv('lams_energy_predict.csv', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIllsOoJsMKx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLOg9ay0sMKx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCFLTWU5sMKx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dispersion_fitting.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}